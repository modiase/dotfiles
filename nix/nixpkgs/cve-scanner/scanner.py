"""CVE scanning via NVD API and NixOS package extraction."""

import asyncio
import json
import subprocess
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Final
from urllib.parse import quote

import aiohttp
from loguru import logger

NVD_API: Final = "https://services.nvd.nist.gov/rest/json/cves/2.0"
REQUEST_TIMEOUT: Final = aiohttp.ClientTimeout(total=30, connect=10)

NOT_FOUND_TTL: Final = 2592000  # 30 days in seconds
STRIP_SUFFIXES: Final = (
    "-wrapped",
    "-full",
    "-minimal",
    "-tiny",
    "-small",
    "-unwrapped",
)
NOT_FOUND_FILE: Final = "nvd-not-found.txt"


def load_not_found_cache(state_dir: Path) -> dict[str, float]:
    """Load cache of products that returned 404 from NVD."""
    cache_file = state_dir / NOT_FOUND_FILE
    if not cache_file.exists():
        return {}

    cache: dict[str, float] = {}
    now = time.time()

    for line in cache_file.read_text().strip().split("\n"):
        if not line or "\t" not in line:
            continue
        product, ts_str = line.split("\t", 1)
        try:
            ts = float(ts_str)
            if now - ts < NOT_FOUND_TTL:
                cache[product] = ts
        except ValueError:
            continue

    return cache


def save_not_found_cache(state_dir: Path, cache: dict[str, float]) -> None:
    """Save the not-found cache to disk."""
    cache_file = state_dir / NOT_FOUND_FILE
    lines = [f"{product}\t{ts}" for product, ts in cache.items()]
    cache_file.write_text("\n".join(lines) + "\n" if lines else "")


@dataclass(frozen=True)
class CVE:
    id: str
    score: float
    description: str
    product: str
    systems: frozenset[str] = field(default_factory=frozenset)


@dataclass
class RateLimiter:
    tokens_per_period: int
    period_seconds: float
    tokens: float = field(init=False)
    last_update: float = field(init=False)

    def __post_init__(self):
        self.tokens = float(self.tokens_per_period)
        self.last_update = time.monotonic()

    async def acquire(self) -> None:
        while True:
            now = time.monotonic()
            elapsed = now - self.last_update
            self.tokens = min(
                self.tokens_per_period,
                self.tokens + elapsed * (self.tokens_per_period / self.period_seconds),
            )
            self.last_update = now

            if self.tokens >= 1.0:
                self.tokens -= 1.0
                logger.debug(f"Token acquired, {self.tokens:.1f} remaining")
                return

            wait_time = (1.0 - self.tokens) * (
                self.period_seconds / self.tokens_per_period
            )
            logger.trace(f"Rate limited, waiting {wait_time:.2f}s for token")
            await asyncio.sleep(wait_time)


def create_rate_limiter(has_api_key: bool) -> RateLimiter:
    if has_api_key:
        logger.info("Using NVD API key (50 req/30s limit)")
        return RateLimiter(tokens_per_period=50, period_seconds=30)
    logger.info("No API key (5 req/30s limit)")
    return RateLimiter(tokens_per_period=5, period_seconds=30)


async def query_nvd(
    session: aiohttp.ClientSession,
    product: str,
    last_check: str,
    api_key: str | None,
    rate_limiter: RateLimiter,
) -> dict | None:
    encoded_product = quote(product, safe="")
    now = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S.000")
    url = f"{NVD_API}?keywordSearch={encoded_product}&pubStartDate={last_check}&pubEndDate={now}"

    headers = {"Accept": "application/json"}
    if api_key:
        headers["apiKey"] = api_key

    logger.debug(f"Request URL: {url}")

    max_retries = 5
    base_delay = 1.0

    for attempt in range(max_retries):
        await rate_limiter.acquire()
        if attempt == 0:
            logger.info(f"Querying NVD for: {product}")

        try:
            async with session.get(
                url, headers=headers, timeout=REQUEST_TIMEOUT
            ) as resp:
                logger.debug(f"Response status for {product}: {resp.status}")
                if resp.status == 200:
                    data = await resp.json()
                    vuln_count = len(data.get("vulnerabilities", []))
                    logger.debug(f"Found {vuln_count} vulnerabilities for {product}")
                    return data
                elif resp.status in (403, 429):
                    retry_after = resp.headers.get("Retry-After")
                    if retry_after:
                        try:
                            delay = float(retry_after)
                        except ValueError:
                            delay = base_delay * (2**attempt)
                        logger.warning(
                            f"Rate limited ({resp.status}) for {product}, "
                            f"retrying in {delay:.1f}s (Retry-After: {retry_after})"
                        )
                    else:
                        delay = base_delay * (2**attempt)
                        logger.warning(
                            f"Rate limited ({resp.status}) for {product}, "
                            f"retrying in {delay:.1f}s"
                        )
                    await asyncio.sleep(delay)
                    continue
                elif resp.status == 404:
                    logger.debug(f"No NVD data for {product}")
                    return None
                elif resp.status >= 500:
                    logger.warning(
                        f"Server error {resp.status} for {product}, retrying..."
                    )
                else:
                    body = await resp.text()
                    logger.error(
                        f"Unexpected status {resp.status} for {product}: {body[:200]}"
                    )
                    return {}
        except aiohttp.ClientError:
            logger.exception(f"Request error for {product}")

        delay = base_delay * (2**attempt) + (asyncio.get_event_loop().time() % 1)
        logger.info(f"Retry {attempt + 1}/{max_retries} in {delay:.1f}s")
        await asyncio.sleep(delay)

    logger.error(f"Failed to query NVD for {product} after {max_retries} retries")
    return {}


def extract_cves(data: dict, cvss_threshold: float, product: str) -> tuple[CVE, ...]:
    results = []

    for vuln in data.get("vulnerabilities", ()):
        cve = vuln.get("cve", {})
        metrics = cve.get("metrics", {})

        score = None
        for metric_key in ("cvssMetricV31", "cvssMetricV30"):
            metric_list = metrics.get(metric_key, ())
            if metric_list:
                score = metric_list[0].get("cvssData", {}).get("baseScore")
                if score:
                    break

        cve_id = cve.get("id")
        if score and score >= cvss_threshold:
            descriptions = cve.get("descriptions", ())
            description = descriptions[0].get("value", "") if descriptions else ""
            results.append(
                CVE(id=cve_id, score=score, description=description, product=product)
            )
            logger.debug(f"Critical CVE found: {cve_id} (CVSS {score})")
        elif score:
            logger.debug(f"CVE {cve_id} below threshold (CVSS {score})")

    return tuple(results)


async def scan_product(
    session: aiohttp.ClientSession,
    product: str,
    *,
    last_check: str,
    api_key: str | None,
    rate_limiter: RateLimiter,
    cvss_threshold: float,
) -> tuple[tuple[CVE, ...], bool]:
    """Returns (cves, was_not_found)."""
    data = await query_nvd(session, product, last_check, api_key, rate_limiter)
    if data is None:
        return ((), True)
    return (extract_cves(data, cvss_threshold, product), False)


async def scan_products(
    products: tuple[str, ...],
    *,
    last_check: str,
    api_key: str | None,
    cvss_threshold: float = 9.0,
    state_dir: Path | None = None,
    refresh_cache: bool = False,
) -> tuple[CVE, ...]:
    rate_limiter = create_rate_limiter(has_api_key=bool(api_key))
    all_cves: list[CVE] = []

    not_found_cache: dict[str, float] = {}
    if state_dir and not refresh_cache:
        not_found_cache = load_not_found_cache(state_dir)

    products_to_scan = tuple(p for p in products if p not in not_found_cache)
    skipped = len(products) - len(products_to_scan)
    if skipped > 0:
        logger.info(f"Skipping {skipped} products cached as not found in NVD")

    total = len(products_to_scan)
    completed = [0]
    new_not_found: list[str] = []

    async def progress_reporter() -> None:
        while True:
            await asyncio.sleep(10)
            logger.debug(f"Progress: {completed[0]}/{total} products scanned")

    async def scan_and_track(
        session: aiohttp.ClientSession, product: str
    ) -> tuple[CVE, ...]:
        cves, was_not_found = await scan_product(
            session,
            product,
            last_check=last_check,
            api_key=api_key,
            rate_limiter=rate_limiter,
            cvss_threshold=cvss_threshold,
        )
        if was_not_found:
            new_not_found.append(product)
        completed[0] += 1
        return cves

    reporter = asyncio.create_task(progress_reporter())

    try:
        async with aiohttp.ClientSession() as session:
            tasks = [scan_and_track(session, product) for product in products_to_scan]
            results = await asyncio.gather(*tasks)
            for cves in results:
                all_cves.extend(cves)
    finally:
        reporter.cancel()
        try:
            await reporter
        except asyncio.CancelledError:
            pass

    if state_dir and new_not_found:
        now = time.time()
        for product in new_not_found:
            not_found_cache[product] = now
        save_not_found_cache(state_dir, not_found_cache)
        logger.info(f"Cached {len(new_not_found)} new not-found products")

    return tuple(all_cves)


def strip_package_suffix(name: str) -> str:
    """Strip common Nix package suffixes."""
    for suffix in STRIP_SUFFIXES:
        if name.endswith(suffix):
            return name[: -len(suffix)]
    return name


def eval_packages(flake_ref: str, attr: str) -> set[str]:
    """Evaluate a Nix attribute and extract package names."""
    result = subprocess.run(
        (
            "nix",
            "eval",
            "--json",
            attr,
            "--apply",
            "map (p: p.pname or p.name or null)",
        ),
        capture_output=True,
        text=True,
        errors="replace",
    )

    if result.returncode != 0:
        logger.debug(f"Failed to eval {attr}: {result.stderr.strip()}")
        return set()

    try:
        names = json.loads(result.stdout)
        return {strip_package_suffix(n) for n in names if n}
    except json.JSONDecodeError:
        logger.warning(f"Invalid JSON from nix eval: {result.stdout[:200]}")
        return set()


def get_system_packages(
    flake_ref: str, systems: tuple[str, ...]
) -> dict[str, tuple[str, ...]]:
    """Extract packages per system using nix eval."""
    system_packages: dict[str, tuple[str, ...]] = {}

    for system in systems:
        logger.info(f"Evaluating packages for {system}...")
        packages: set[str] = set()

        sys_attr = f"{flake_ref}#nixosConfigurations.{system}.config.environment.systemPackages"
        packages.update(eval_packages(flake_ref, sys_attr))

        home_attr = f"{flake_ref}#homeConfigurations.moye-{system}.config.home.packages"
        packages.update(eval_packages(flake_ref, home_attr))

        system_packages[system] = tuple(sorted(packages))
        logger.info(f"Found {len(packages)} packages in {system}")

    return system_packages


def get_package_to_systems(
    system_packages: dict[str, tuple[str, ...]],
) -> dict[str, frozenset[str]]:
    """Build reverse mapping from package name to systems containing it."""
    pkg_to_systems: dict[str, set[str]] = {}

    for system, packages in system_packages.items():
        for pkg in packages:
            if pkg not in pkg_to_systems:
                pkg_to_systems[pkg] = set()
            pkg_to_systems[pkg].add(system)

    return {pkg: frozenset(systems) for pkg, systems in pkg_to_systems.items()}
