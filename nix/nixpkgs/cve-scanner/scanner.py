"""CVE scanning via NVD API and NixOS package extraction."""

import asyncio
import re
import subprocess
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Final
from urllib.parse import quote

import aiohttp
from loguru import logger

NVD_API: Final = "https://services.nvd.nist.gov/rest/json/cves/2.0"
REQUEST_TIMEOUT: Final = aiohttp.ClientTimeout(total=30, connect=10)

NON_PACKAGE_PATTERNS: Final = (
    r"^\d+",
    r"^X-",
    r"^Xresources",
    r"\.sh$",
    r"\.conf$",
    r"\.service$",
    r"\.socket$",
    r"\.timer$",
    r"\.target$",
    r"\.mount$",
    r"^hook-",
    r"^wrap-",
    r"^setup-",
    r"^builder",
    r"^auto-fix-",
    r"^move-",
    r"^patch-",
    r"^strip-",
    r"^compress-",
    r"^multiple-outputs",
    r"-authorized_keys$",
    r"-environment$",
    r"-system-path$",
    r"-etc$",
    r"-firmware$",
    r"-events$",
)

STRIP_SUFFIXES: Final = ("-dev", "-doc", "-man", "-info", "-lib", "-bin", "-out")

VERSION_START: Final = re.compile(r"^\d")


@dataclass(frozen=True)
class CVE:
    id: str
    score: float
    description: str
    product: str


@dataclass
class RateLimiter:
    tokens_per_period: int
    period_seconds: float
    tokens: float = field(init=False)
    last_update: float = field(init=False)

    def __post_init__(self):
        self.tokens = float(self.tokens_per_period)
        self.last_update = time.monotonic()

    async def acquire(self) -> None:
        while True:
            now = time.monotonic()
            elapsed = now - self.last_update
            self.tokens = min(
                self.tokens_per_period,
                self.tokens + elapsed * (self.tokens_per_period / self.period_seconds),
            )
            self.last_update = now

            if self.tokens >= 1.0:
                self.tokens -= 1.0
                logger.debug(f"Token acquired, {self.tokens:.1f} remaining")
                return

            wait_time = (1.0 - self.tokens) * (
                self.period_seconds / self.tokens_per_period
            )
            logger.trace(f"Rate limited, waiting {wait_time:.2f}s for token")
            await asyncio.sleep(wait_time)


def create_rate_limiter(has_api_key: bool) -> RateLimiter:
    if has_api_key:
        logger.info("Using NVD API key (50 req/30s limit)")
        return RateLimiter(tokens_per_period=50, period_seconds=30)
    logger.info("No API key (5 req/30s limit)")
    return RateLimiter(tokens_per_period=5, period_seconds=30)


async def query_nvd(
    session: aiohttp.ClientSession,
    product: str,
    last_check: str,
    api_key: str | None,
    rate_limiter: RateLimiter,
) -> dict:
    encoded_product = quote(product, safe="")
    now = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S.000")
    url = f"{NVD_API}?keywordSearch={encoded_product}&pubStartDate={last_check}&pubEndDate={now}"

    headers = {"Accept": "application/json"}
    if api_key:
        headers["apiKey"] = api_key

    logger.debug(f"Request URL: {url}")

    max_retries = 5
    base_delay = 1.0

    for attempt in range(max_retries):
        await rate_limiter.acquire()

        try:
            async with session.get(
                url, headers=headers, timeout=REQUEST_TIMEOUT
            ) as resp:
                logger.debug(f"Response status for {product}: {resp.status}")
                if resp.status == 200:
                    data = await resp.json()
                    vuln_count = len(data.get("vulnerabilities", []))
                    logger.debug(f"Found {vuln_count} vulnerabilities for {product}")
                    return data
                elif resp.status in (403, 429):
                    retry_after = resp.headers.get("Retry-After")
                    if retry_after:
                        try:
                            delay = float(retry_after)
                        except ValueError:
                            delay = base_delay * (2**attempt)
                        logger.warning(
                            f"Rate limited ({resp.status}) for {product}, "
                            f"retrying in {delay:.1f}s (Retry-After: {retry_after})"
                        )
                    else:
                        delay = base_delay * (2**attempt)
                        logger.warning(
                            f"Rate limited ({resp.status}) for {product}, "
                            f"retrying in {delay:.1f}s"
                        )
                    await asyncio.sleep(delay)
                    continue
                elif resp.status >= 500:
                    logger.warning(
                        f"Server error {resp.status} for {product}, retrying..."
                    )
                else:
                    body = await resp.text()
                    logger.error(
                        f"Unexpected status {resp.status} for {product}: {body[:200]}"
                    )
                    return {}
        except aiohttp.ClientError:
            logger.exception(f"Request error for {product}")

        delay = base_delay * (2**attempt) + (asyncio.get_event_loop().time() % 1)
        logger.info(f"Retry {attempt + 1}/{max_retries} in {delay:.1f}s")
        await asyncio.sleep(delay)

    logger.error(f"Failed to query NVD for {product} after {max_retries} retries")
    return {}


def extract_cves(data: dict, cvss_threshold: float, product: str) -> tuple[CVE, ...]:
    results = []

    for vuln in data.get("vulnerabilities", ()):
        cve = vuln.get("cve", {})
        metrics = cve.get("metrics", {})

        score = None
        for metric_key in ("cvssMetricV31", "cvssMetricV30"):
            metric_list = metrics.get(metric_key, ())
            if metric_list:
                score = metric_list[0].get("cvssData", {}).get("baseScore")
                if score:
                    break

        cve_id = cve.get("id")
        if score and score >= cvss_threshold:
            descriptions = cve.get("descriptions", ())
            description = descriptions[0].get("value", "") if descriptions else ""
            results.append(
                CVE(id=cve_id, score=score, description=description, product=product)
            )
            logger.debug(f"Critical CVE found: {cve_id} (CVSS {score})")
        elif score:
            logger.debug(f"CVE {cve_id} below threshold (CVSS {score})")

    return tuple(results)


async def scan_product(
    session: aiohttp.ClientSession,
    product: str,
    *,
    last_check: str,
    api_key: str | None,
    rate_limiter: RateLimiter,
    cvss_threshold: float,
) -> tuple[CVE, ...]:
    logger.info(f"Querying NVD for: {product}")
    data = await query_nvd(session, product, last_check, api_key, rate_limiter)
    return extract_cves(data, cvss_threshold, product)


async def scan_products(
    products: tuple[str, ...],
    *,
    last_check: str,
    api_key: str | None,
    cvss_threshold: float = 9.0,
) -> tuple[CVE, ...]:
    rate_limiter = create_rate_limiter(has_api_key=bool(api_key))
    all_cves: list[CVE] = []
    total = len(products)
    completed = [0]

    async def progress_reporter() -> None:
        while True:
            await asyncio.sleep(10)
            logger.debug(f"Progress: {completed[0]}/{total} products scanned")

    async def scan_and_track(
        session: aiohttp.ClientSession, product: str
    ) -> tuple[CVE, ...]:
        result = await scan_product(
            session,
            product,
            last_check=last_check,
            api_key=api_key,
            rate_limiter=rate_limiter,
            cvss_threshold=cvss_threshold,
        )
        completed[0] += 1
        return result

    reporter = asyncio.create_task(progress_reporter())

    try:
        async with aiohttp.ClientSession() as session:
            tasks = [scan_and_track(session, product) for product in products]
            results = await asyncio.gather(*tasks)
            for cves in results:
                all_cves.extend(cves)
    finally:
        reporter.cancel()
        try:
            await reporter
        except asyncio.CancelledError:
            pass

    return tuple(all_cves)


def parse_store_path(store_path: str) -> tuple[str, str | None] | None:
    """Parse /nix/store/hash-name-version into (name, version).

    Examples:
      /nix/store/abc123-acl-2.3.2 -> ("acl", "2.3.2")
      /nix/store/abc123-alsa-firmware-1.2.4-zstd -> ("alsa-firmware", "1.2.4-zstd")
    """
    if not store_path or not store_path.startswith("/nix/store/"):
        return None

    basename = store_path.split("/")[-1]
    parts = basename.split("-", 1)
    if len(parts) < 2:
        return None

    name_version = parts[1]

    for pattern in NON_PACKAGE_PATTERNS:
        if re.search(pattern, name_version):
            return None

    segments = name_version.split("-")
    name_parts = []
    version = None

    for i, segment in enumerate(segments):
        if VERSION_START.match(segment):
            version = "-".join(segments[i:])
            break
        name_parts.append(segment)

    name = "-".join(name_parts) if name_parts else name_version

    for suffix in STRIP_SUFFIXES:
        if name.endswith(suffix):
            name = name[: -len(suffix)]
            break

    if not name or len(name) < 2:
        return None

    return (name, version)


def get_system_closure(flake_ref: str, system: str) -> tuple[str, ...]:
    attr = f"{flake_ref}#nixosConfigurations.{system}.config.system.build.toplevel"
    logger.debug(f"Getting closure for {attr}")

    result = subprocess.run(
        ("nix", "path-info", "-r", attr),
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        logger.warning(f"Failed to get closure for {system}: {result.stderr}")
        return ()

    paths = tuple(result.stdout.strip().split("\n"))
    logger.debug(f"Found {len(paths)} paths in {system} closure")
    return paths


def get_unique_packages(flake_ref: str, systems: tuple[str, ...]) -> tuple[str, ...]:
    all_packages: set[str] = set()

    for system in systems:
        logger.info(f"Extracting packages from {system}...")
        paths = get_system_closure(flake_ref, system)

        for path in paths:
            result = parse_store_path(path)
            if result:
                all_packages.add(result[0])

        logger.info(f"Found {len(all_packages)} unique packages so far")

    sorted_packages = tuple(sorted(all_packages))
    logger.info(f"Total unique packages: {len(sorted_packages)}")
    return sorted_packages
